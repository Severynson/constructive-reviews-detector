# ğŸ§  ğŸ†š ğŸ­ constructive-reviews-detector

This project builds an end-to-end pipeline for **detecting constructive customer reviews** â€” reviews that contain **specific, actionable feedback** that a business can use to improve (e.g. â€œservice was slowâ€, â€œmusic was too loudâ€, â€œburger was undercookedâ€), as opposed to purely **emotional or vague** comments (e.g. â€œworst place everâ€, â€œamazing!!â€).

â¸»

## ğŸ—‚ï¸ It consists of:

1. **Data collection** from TripAdvisor (via API)
2. **LLM-based labeling** (OpenAI) of reviews as constructive vs. non-constructive
3. **Supervised training** of a BERT-style classifier on the labeled dataset
4. **Local inference** using the fine-tuned classifier

All TripAdvisor reviews in this project were collected by me and are stored in the `data/` folder (see [Dataset](#dataset) section).

â¸»

## ğŸ¯ Current Test set Metrics
**Cross-Entropy Loss:** `0.3405`  
**Accuracy:** `0.8824`
**F1 Score:** `0.8800`

â¸»

## ğŸ“‚ Project structure

```
constructive-reviews-detector/
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ environment.yml           # Conda environment definition
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ tripadvisor_reviews.csv          # raw, unlabeled reviews from TripAdvisor API
â”‚   â”œâ”€â”€ tripadvisor_reviews_labeled.csv  # same, but labeled by OpenAI
â”‚   â”œâ”€â”€ reviews.tsv                      # trimmed, balanced dataset (review + constructive)
â”‚   â””â”€â”€ to_check_locations.tsv           # list of locations to fetch from TripAdvisor
â”œâ”€â”€ models/
â”‚   â””â”€â”€ ... (fine-tuned classifier, e.g. models/review_classifier/)
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ data_preprocessing.ipynb  # cleaning, balancing, preparing data for training
â”‚   â”œâ”€â”€ model_training.ipynb      # fine-tuning BERT on labeled data
â”‚   â””â”€â”€ model_inference.ipynb     # examples of running inference
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ data_collector.py   # pulls reviews from TripAdvisor API
â”‚   â””â”€â”€ review_labler.py    # labels reviews as constructive vs. not using OpenAI
â””â”€â”€ src/
    â””â”€â”€ model.py            # ReviewClassifier wrapper around Hugging Face model
```

â¸»

## ğŸ“Š Dataset

All data in data/ was collected by me specifically for this project.

The three most important files:

- `data/tripadvisor_reviews.csv`
Raw, unlabeled TripAdvisor reviews pulled via the TripAdvisor Content API.
Each row includes location info, rating, title, text, language, user name, etc.

- `data/tripadvisor_reviews_labeled.csv` - Same reviews, but with additional columns:
    - constructive â€“ 1 if the LLM judged the review to contain actionable, specific feedback, 0 otherwise
    - llm_logic_log â€“ the LLMâ€™s short explanation of its decision

- `data/reviews.tsv` - A trimmed and cleaned dataset containing only:
    - review â€“ combined title+text (and possibly some light context)
    - constructive â€“ 0 or 1

This dataset is also balanced: the number of constructive reviews (constructive = 1) is approximately equal to the number of purely sentimental / non-constructive reviews (constructive = 0).
It is used as the main input for training and evaluation of the BERT-style classifier.
â„¹ï¸ Note: TripAdvisorâ€™s Content API only returns up to 5 most recent reviews per location, so data/to_check_locations.tsv determines which locations were queried.

â¸»

## ğŸ› ï¸ Installation & environment

The project is designed to be reproducible via Conda.

1. Create the environment
```
conda env create -f environment.yml
```

2. Activate it
```
conda activate constructive-reviews-detector
```

â¸»

## ğŸŒ 1. Data collection from TripAdvisor

Script: scripts/data_collector.py

This script:

- Reads a list of locations from data/to_check_locations.tsv with columns:
    - `title`
    - `url` (TripAdvisor location page)
- Extracts `location_id` from the URL (via the `-dXXXXX-` pattern)
- Calls the TripAdvisor Content API to fetch up to 5 most recent reviews per location
- Filters/normalizes text and writes reviews to `data/tripadvisor_reviews.csv` (pipe `|` delimited)
- Tracks already-processed locations in `data/checked_locations.tsv`

## ğŸ”‘ Required environment variable

Define in .env:
```TRIPADVISOR_API_KEY=your_tripadvisor_content_api_key```

## â–¶ï¸ Run the collector
```
conda activate constructive-reviews-detector
python scripts/data_collector.py
```

Outputs:

- `data/tripadvisor_reviews.csv` â€“ raw, unlabeled reviews
- `data/checked_locations.tsv` â€“ locations that have already been processed

â¸»

## ğŸ¤– 2. Labeling reviews with OpenAI

Script: `scripts/review_labler.py`

This script:

- Reads `data/tripadvisor_reviews.csv` (pipe `|` delimited)
- Sends batched reviews to an OpenAI model (default: `gpt-5.1`)
- Uses a strict system prompt to decide for each review:
    - `1` â€“ constructive / contains specific, actionable feedback
    - `0` â€“ purely sentimental / vague / not actionable
- Writes results to data/tripadvisor_reviews_labeled.csv, adding:
    - `constructive`
    - `llm_logic_log`

## ğŸ”‘ Required environment variable

Add to `.env`:

```
OPENAI_API_KEY=your_openai_api_key
```

## â–¶ï¸ Run the labeling script
```
python scripts/review_labler.py \
  --input data/tripadvisor_reviews.csv \
  --output data/tripadvisor_reviews_labeled.csv \
  --model gpt-5.1
```
Useful flags:
- `--relabel` â€“ re-label rows even if `constructive` already exists
- `--limit N` â€“ process only the first N rows
- `--sleep S` â€“ sleep `S` seconds between API calls to avoid rate limits
See all options:
```
python scripts/review_labler.py --help
```

â¸»

## ğŸ“œ 3. AI classifier prompt (LLM labeling criteria)
The full system prompt that defines what â€œconstructiveâ€ means is stored in:
- `scripts/review_labler.py` â†’ SYSTEM_PROMPT
In short, the LLM returns 1 only if the review includes concrete, factual, operational details, such as:
- Speed of service
- Staff behavior, attentiveness, knowledge, or errors
- Specific food/drink quality details (temperature, texture, seasoning, doneness, etc.)
- Environment, comfort, noise level, cleanliness
- Timing/pacing issues, portion size differences, order accuracy
- Operational / logistics problems (billing issues, reservation failures, delivery problems, etc.)
- Accessibility, safety concerns, menu clarity, parking/entrance issues, and so on

The model returns 0 when the review is:
- Vague (â€œfood was bad/goodâ€, â€œservice needs improvementâ€)
- Purely emotional (â€œwe were so disappointedâ€, â€œamazing vibeâ€)
- Just storytelling / narrative without actionable content
- General praise or criticism without specifying what exactly was good or bad
When in doubt, the prompt instructs the LLM to choose 0.

For exact wording (which is important if you want to reproduce labeling), please refer directly to `scripts/review_labler.py`.

â¸»

## ğŸ§¬ 4. Training the BERT-style classifier

Core class: `src/model.py` â†’ `ReviewClassifier`
Notebooks: `notebooks/data_preprocessing.ipynb`, `notebooks/model_training.ipynb`

The classifier is built on a Hugging Face backbone (`bert-base-uncased`) and wrapped in a small helper class that:
- Loads the tokenizer and model from `transformers`
- Converts a pandas DataFrame (like `data/reviews.tsv`) to a tokenized Hugging Face Dataset
- Supports `.predict(text, threshold=...)` for local inference
The training process (implemented in `notebooks/model_training.ipynb`) typically does:
1. Load `data/reviews.tsv` (columns: `review`, `constructive`).
2. Split into train / validation / test sets (e.g. using `train_test_split`).
3. Use `ReviewClassifier.df_to_dataset(...)` to turn DataFrames into tokenized datasets.
4. Fine-tune the model with `transformers.Trainer` + `TrainingArguments`.
5. Save the best model and tokenizer into `models/review_classifier/` using the Trainer API.
For detailed training code and hyperparameters, check the `model_training.ipynb` notebook.

â¸»

## ğŸ” 5. Running inference with the fine-tuned model
Files involved:
- Fine-tuned model directory: `models/review_classifier/`
- Helper class: src/model.py â†’ `ReviewClassifier`
- Examples: `notebooks/model_inference.ipynb`

Typical usage:
1. Load the fine-tuned model using `ReviewClassifier.from_finetuned("models/review_classifier")`.
2. Call `.predict(text, threshold=...)` for a single review.
3. Interpret:
    - the returned label (`1` = constructive, `0` = non-constructive)
    - the probability of class 1 (constructive) returned alongside the label
The `model_inference.ipynb` notebook contains concrete examples of loading the model, running predictions, and interpreting outputs.

â¸»

## ğŸ” 6. Reproducing the full pipeline

To recreate the entire process from raw locations â†’ labeled dataset â†’ trained classifier:
1. Set up environment
```
conda env create -f environment.yml
conda activate constructive-reviews-detector
```
2. Prepare TripAdvisor locations
- Add locations to `data/to_check_locations.tsv` with columns `title` and `url`.
3. Collect reviews
- Ensure `.env` contains `TRIPADVISOR_API_KEY`.
Run:
```
python scripts/data_collector.py
```
- This produces `data/tripadvisor_reviews.csv`.
4. Label reviews with OpenAI
- Ensure `.env` contains `OPENAI_API_KEY`.
- Run:
```
python scripts/review_labler.py \
  --input data/tripadvisor_reviews.csv \
  --output data/tripadvisor_reviews_labeled.csv
```
5. Prepare final training dataset
- Use `notebooks/data_preprocessing.ipynb` to:
    - clean and merge title+text
    - create `data/reviews.tsv` with `review` + `constructive`
    - balance the classes (roughly 50/50 constructive vs. non-constructive)
6. Train the classifier
- Run notebooks/model_training.ipynb to:
    - load `data/reviews.tsv`
    - split into train / val / test
    - fine-tune the model
    - save it under `models/review_classifier/`
7. Run inference
- Use `notebooks/model_inference.ipynb` or your own script that:
    - imports `ReviewClassifier` from `src/model.py`
    - loads the model from `models/review_classifier/`
    - calls `.predict()` on your own review texts

â¸»

## ğŸ“œ License:
MIT licence.
